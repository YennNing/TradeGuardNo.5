{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b6fa719c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.9.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_datareader.data as web\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "b379114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 128\n",
    "\n",
    "d_k = 256\n",
    "d_v = 256\n",
    "n_heads = 12\n",
    "ff_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "85545a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Open</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-03</th>\n",
       "      <td>15960.00</td>\n",
       "      <td>12600.00</td>\n",
       "      <td>12920.00</td>\n",
       "      <td>15600.00</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-04</th>\n",
       "      <td>18720.00</td>\n",
       "      <td>15960.00</td>\n",
       "      <td>15960.00</td>\n",
       "      <td>16760.00</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-05</th>\n",
       "      <td>17920.00</td>\n",
       "      <td>16040.00</td>\n",
       "      <td>17920.00</td>\n",
       "      <td>16800.00</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-06</th>\n",
       "      <td>17880.00</td>\n",
       "      <td>16000.00</td>\n",
       "      <td>16560.00</td>\n",
       "      <td>17200.00</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-01-09</th>\n",
       "      <td>18000.00</td>\n",
       "      <td>16720.00</td>\n",
       "      <td>17720.00</td>\n",
       "      <td>17120.00</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-13</th>\n",
       "      <td>1.54</td>\n",
       "      <td>1.48</td>\n",
       "      <td>1.52</td>\n",
       "      <td>1.51</td>\n",
       "      <td>69700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-14</th>\n",
       "      <td>1.53</td>\n",
       "      <td>1.49</td>\n",
       "      <td>1.53</td>\n",
       "      <td>1.50</td>\n",
       "      <td>44600.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-15</th>\n",
       "      <td>1.65</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1.62</td>\n",
       "      <td>481000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-18</th>\n",
       "      <td>1.73</td>\n",
       "      <td>1.63</td>\n",
       "      <td>1.67</td>\n",
       "      <td>1.64</td>\n",
       "      <td>297300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-07-19</th>\n",
       "      <td>1.89</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.69</td>\n",
       "      <td>1.89</td>\n",
       "      <td>341400.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2653 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                High       Low      Open     Close    Volume\n",
       "Date                                                        \n",
       "2012-01-03  15960.00  12600.00  12920.00  15600.00       8.0\n",
       "2012-01-04  18720.00  15960.00  15960.00  16760.00      16.0\n",
       "2012-01-05  17920.00  16040.00  17920.00  16800.00       4.0\n",
       "2012-01-06  17880.00  16000.00  16560.00  17200.00       1.0\n",
       "2012-01-09  18000.00  16720.00  17720.00  17120.00       3.0\n",
       "...              ...       ...       ...       ...       ...\n",
       "2022-07-13      1.54      1.48      1.52      1.51   69700.0\n",
       "2022-07-14      1.53      1.49      1.53      1.50   44600.0\n",
       "2022-07-15      1.65      1.50      1.50      1.62  481000.0\n",
       "2022-07-18      1.73      1.63      1.67      1.64  297300.0\n",
       "2022-07-19      1.89      1.64      1.69      1.89  341400.0\n",
       "\n",
       "[2653 rows x 5 columns]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_ticket = \"GLBS\"\n",
    "start = datetime.datetime(2012,1,1)\n",
    "end = datetime.datetime(2022,7,19)\n",
    "df = web.DataReader(stock_ticket,\"yahoo\",start,end)\n",
    "df.drop(columns=['Adj Close'], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "509d69f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sc = MinMaxScaler()\n",
    "train_set = x_sc.fit_transform(df.values)\n",
    "y_sc = MinMaxScaler()\n",
    "test_set = y_sc.fit_transform(df['Close'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "22498f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1724, 5)\n",
      "Validation data shape: (398, 5)\n",
      "Test data shape: (531, 5)\n"
     ]
    }
   ],
   "source": [
    "last_10pct = int(len(df)*0.8)\n",
    "last_20pct = int(len(df)*0.65) # Last 20% of series\n",
    "# Convert pandas columns into arrays\n",
    "train_data =train_set[:last_20pct]\n",
    "val_data = train_set[last_20pct:last_10pct]\n",
    "test_data = train_set[last_10pct:]\n",
    "print('Training data shape: {}'.format(train_data.shape))\n",
    "print('Validation data shape: {}'.format(val_data.shape))\n",
    "print('Test data shape: {}'.format(test_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "f406e9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1635, 60, 5) (1635, 30)\n",
      "(309, 60, 5) (309, 30)\n"
     ]
    }
   ],
   "source": [
    "# Training data\n",
    "n_future = 30\n",
    "X_train, y_train = [], []\n",
    "for i in range(seq_len, len(train_data)-n_future+1):\n",
    "    X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n",
    "    y_train.append(train_data[i:i+n_future, 3]) #Value of 4th column (Close Price) of df-row 128+1\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Validation data\n",
    "X_val, y_val = [], []\n",
    "for i in range(seq_len, len(val_data)-n_future+1):\n",
    "    X_val.append(val_data[i-seq_len:i])\n",
    "    y_val.append(val_data[i:i+n_future, 3])\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "\n",
    "###############################################################################\n",
    "\n",
    "# Test data\n",
    "X_test, y_test = [], []\n",
    "for i in range(seq_len, len(test_data)-n_future+1):\n",
    "    X_test.append(test_data[i-seq_len:i])\n",
    "    y_test.append(test_data[i:i+n_future, 3])    \n",
    "X_test, y_test = np.array(X_test), np.array(y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "bd726534",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "    \n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                shape=(int(self.seq_len),),\n",
    "                                initializer='uniform',\n",
    "                                trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
    "        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "    \n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "   \n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e958fad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(Layer):\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k, \n",
    "                       input_shape=input_shape, \n",
    "                       kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "    \n",
    "        self.key = Dense(self.d_k, \n",
    "                     input_shape=input_shape, \n",
    "                     kernel_initializer='glorot_uniform', \n",
    "                     bias_initializer='glorot_uniform')\n",
    "    \n",
    "        self.value = Dense(self.d_v, \n",
    "                       input_shape=input_shape, \n",
    "                       kernel_initializer='glorot_uniform', \n",
    "                       bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "    \n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out    \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "    \n",
    "    # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "        self.linear = Dense(input_shape[0][-1], \n",
    "                        input_shape=input_shape, \n",
    "                        kernel_initializer='glorot_uniform', \n",
    "                        bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear   \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
    "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
    "  \n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer \n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                   'd_v': self.d_v,\n",
    "                   'n_heads': self.n_heads,\n",
    "                   'ff_dim': self.ff_dim,\n",
    "                   'attn_heads': self.attn_heads,\n",
    "                   'dropout_rate': self.dropout_rate})\n",
    "        return config         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "0dcf7836",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_11 (InputLayer)          [(None, 60, 5)]      0           []                               \n",
      "                                                                                                  \n",
      " time2_vector_28 (Time2Vector)  (None, 60, 2)        240         ['input_11[0][0]']               \n",
      "                                                                                                  \n",
      " concatenate_10 (Concatenate)   (None, 60, 7)        0           ['input_11[0][0]',               \n",
      "                                                                  'time2_vector_28[0][0]']        \n",
      "                                                                                                  \n",
      " transformer_encoder_84 (Transf  (None, 60, 7)       99114       ['concatenate_10[0][0]',         \n",
      " ormerEncoder)                                                    'concatenate_10[0][0]',         \n",
      "                                                                  'concatenate_10[0][0]']         \n",
      "                                                                                                  \n",
      " transformer_encoder_85 (Transf  (None, 60, 7)       99114       ['transformer_encoder_84[0][0]', \n",
      " ormerEncoder)                                                    'transformer_encoder_84[0][0]', \n",
      "                                                                  'transformer_encoder_84[0][0]'] \n",
      "                                                                                                  \n",
      " transformer_encoder_86 (Transf  (None, 60, 7)       99114       ['transformer_encoder_85[0][0]', \n",
      " ormerEncoder)                                                    'transformer_encoder_85[0][0]', \n",
      "                                                                  'transformer_encoder_85[0][0]'] \n",
      "                                                                                                  \n",
      " global_average_pooling1d_10 (G  (None, 60)          0           ['transformer_encoder_86[0][0]'] \n",
      " lobalAveragePooling1D)                                                                           \n",
      "                                                                                                  \n",
      " dropout_20 (Dropout)           (None, 60)           0           ['global_average_pooling1d_10[0][\n",
      "                                                                 0]']                             \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 64)           3904        ['dropout_20[0][0]']             \n",
      "                                                                                                  \n",
      " dropout_21 (Dropout)           (None, 64)           0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 30)           1950        ['dropout_21[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 303,436\n",
      "Trainable params: 303,436\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0824 - mae: 0.2146 - mape: 112.0585\n",
      "Epoch 1: val_loss improved from inf to 0.00886, saving model to Transformer+TimeEmbedding.hdf5\n",
      "52/52 [==============================] - 58s 542ms/step - loss: 0.0824 - mae: 0.2146 - mape: 112.0585 - val_loss: 0.0089 - val_mae: 0.0927 - val_mape: 1842.2081\n",
      "Epoch 2/20\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0113 - mae: 0.0767 - mape: 58.4222\n",
      "Epoch 2: val_loss improved from 0.00886 to 0.00297, saving model to Transformer+TimeEmbedding.hdf5\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.0113 - mae: 0.0767 - mape: 58.4222 - val_loss: 0.0030 - val_mae: 0.0539 - val_mape: 1077.4032\n",
      "Epoch 3/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0091 - mae: 0.0690 - mape: 51.4236\n",
      "Epoch 3: val_loss improved from 0.00297 to 0.00195, saving model to Transformer+TimeEmbedding.hdf5\n",
      "52/52 [==============================] - 21s 403ms/step - loss: 0.0091 - mae: 0.0690 - mape: 51.3652 - val_loss: 0.0019 - val_mae: 0.0436 - val_mape: 898.8239\n",
      "Epoch 4/20\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0086 - mae: 0.0663 - mape: 50.4560\n",
      "Epoch 4: val_loss did not improve from 0.00195\n",
      "52/52 [==============================] - 23s 445ms/step - loss: 0.0086 - mae: 0.0663 - mape: 50.4560 - val_loss: 0.0020 - val_mae: 0.0443 - val_mape: 902.4240\n",
      "Epoch 5/20\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0082 - mae: 0.0649 - mape: 47.7375\n",
      "Epoch 5: val_loss improved from 0.00195 to 0.00166, saving model to Transformer+TimeEmbedding.hdf5\n",
      "52/52 [==============================] - 22s 421ms/step - loss: 0.0082 - mae: 0.0649 - mape: 47.7375 - val_loss: 0.0017 - val_mae: 0.0404 - val_mape: 825.2410\n",
      "Epoch 6/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0083 - mae: 0.0661 - mape: 48.9935\n",
      "Epoch 6: val_loss improved from 0.00166 to 0.00134, saving model to Transformer+TimeEmbedding.hdf5\n",
      "52/52 [==============================] - 21s 412ms/step - loss: 0.0084 - mae: 0.0662 - mape: 48.9990 - val_loss: 0.0013 - val_mae: 0.0361 - val_mape: 751.3043\n",
      "Epoch 7/20\n",
      "52/52 [==============================] - ETA: 0s - loss: 0.0080 - mae: 0.0641 - mape: 46.8163\n",
      "Epoch 7: val_loss improved from 0.00134 to 0.00082, saving model to Transformer+TimeEmbedding.hdf5\n",
      "52/52 [==============================] - 21s 411ms/step - loss: 0.0080 - mae: 0.0641 - mape: 46.8163 - val_loss: 8.2495e-04 - val_mae: 0.0281 - val_mape: 602.3410\n",
      "Epoch 8/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0079 - mae: 0.0637 - mape: 47.1392\n",
      "Epoch 8: val_loss did not improve from 0.00082\n",
      "52/52 [==============================] - 22s 425ms/step - loss: 0.0079 - mae: 0.0637 - mape: 47.0948 - val_loss: 0.0012 - val_mae: 0.0337 - val_mape: 706.8558\n",
      "Epoch 9/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0080 - mae: 0.0643 - mape: 46.0769\n",
      "Epoch 9: val_loss did not improve from 0.00082\n",
      "52/52 [==============================] - 21s 407ms/step - loss: 0.0080 - mae: 0.0643 - mape: 46.3179 - val_loss: 0.0017 - val_mae: 0.0410 - val_mape: 849.6975\n",
      "Epoch 10/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0074 - mae: 0.0619 - mape: 46.6237\n",
      "Epoch 10: val_loss did not improve from 0.00082\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.0074 - mae: 0.0620 - mape: 46.7617 - val_loss: 0.0011 - val_mae: 0.0329 - val_mape: 692.6384\n",
      "Epoch 11/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0070 - mae: 0.0599 - mape: 43.3662\n",
      "Epoch 11: val_loss did not improve from 0.00082\n",
      "52/52 [==============================] - 21s 395ms/step - loss: 0.0071 - mae: 0.0600 - mape: 43.3142 - val_loss: 0.0011 - val_mae: 0.0334 - val_mape: 699.3360\n",
      "Epoch 12/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0073 - mae: 0.0617 - mape: 47.1116\n",
      "Epoch 12: val_loss improved from 0.00082 to 0.00077, saving model to Transformer+TimeEmbedding.hdf5\n",
      "52/52 [==============================] - 21s 401ms/step - loss: 0.0073 - mae: 0.0617 - mape: 47.0752 - val_loss: 7.7345e-04 - val_mae: 0.0271 - val_mape: 581.3891\n",
      "Epoch 13/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0067 - mae: 0.0584 - mape: 41.2692\n",
      "Epoch 13: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.0067 - mae: 0.0584 - mape: 41.2223 - val_loss: 0.0012 - val_mae: 0.0340 - val_mape: 712.6160\n",
      "Epoch 14/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0065 - mae: 0.0576 - mape: 42.0162\n",
      "Epoch 14: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 20s 390ms/step - loss: 0.0065 - mae: 0.0576 - mape: 41.9955 - val_loss: 0.0011 - val_mae: 0.0325 - val_mape: 684.3061\n",
      "Epoch 15/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0061 - mae: 0.0561 - mape: 41.3063\n",
      "Epoch 15: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 20s 380ms/step - loss: 0.0061 - mae: 0.0560 - mape: 41.3400 - val_loss: 8.7998e-04 - val_mae: 0.0290 - val_mape: 619.0978\n",
      "Epoch 16/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0053 - mae: 0.0512 - mape: 34.9398\n",
      "Epoch 16: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 20s 388ms/step - loss: 0.0053 - mae: 0.0512 - mape: 35.0619 - val_loss: 0.0010 - val_mae: 0.0315 - val_mape: 662.8776\n",
      "Epoch 17/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0052 - mae: 0.0511 - mape: 35.3136\n",
      "Epoch 17: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 20s 378ms/step - loss: 0.0052 - mae: 0.0512 - mape: 35.4415 - val_loss: 8.6893e-04 - val_mae: 0.0290 - val_mape: 616.5520\n",
      "Epoch 18/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0057 - mae: 0.0543 - mape: 41.4971\n",
      "Epoch 18: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 20s 377ms/step - loss: 0.0056 - mae: 0.0543 - mape: 41.5233 - val_loss: 0.0014 - val_mae: 0.0372 - val_mape: 771.9758\n",
      "Epoch 19/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0048 - mae: 0.0502 - mape: 39.4707\n",
      "Epoch 19: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 19s 374ms/step - loss: 0.0049 - mae: 0.0502 - mape: 39.4914 - val_loss: 7.8359e-04 - val_mae: 0.0275 - val_mape: 587.8608\n",
      "Epoch 20/20\n",
      "51/52 [============================>.] - ETA: 0s - loss: 0.0051 - mae: 0.0516 - mape: 39.4822\n",
      "Epoch 20: val_loss did not improve from 0.00077\n",
      "52/52 [==============================] - 20s 381ms/step - loss: 0.0051 - mae: 0.0515 - mape: 39.4962 - val_loss: 0.0026 - val_mae: 0.0504 - val_mape: 1027.5232\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    time_embedding = Time2Vector(seq_len)\n",
    "    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "    in_seq = Input(shape=(seq_len, 5))\n",
    "    x = time_embedding(in_seq)\n",
    "    x = Concatenate(axis=-1)([in_seq, x])\n",
    "    x = attn_layer1((x, x, x))\n",
    "    x = attn_layer2((x, x, x))\n",
    "    x = attn_layer3((x, x, x))\n",
    "    x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(n_future, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=in_seq, outputs=out)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint('Transformer+TimeEmbedding.hdf5', \n",
    "                                              monitor='val_loss', \n",
    "                                              save_best_only=True, \n",
    "                                              verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=20, \n",
    "                    callbacks=[callback],\n",
    "                    validation_data=(X_val, y_val))  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2885c27",
   "metadata": {},
   "source": [
    "# 预测！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "8307f6eb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 21s 141ms/step\n",
      "10/10 [==============================] - 1s 135ms/step\n",
      "14/14 [==============================] - 2s 147ms/step\n",
      " \n",
      "Evaluation metrics\n",
      "Training Data - Loss: 0.0061, MAE: 0.0557, MAPE: 32.3928\n",
      "Validation Data - Loss: 0.0008, MAE: 0.0271, MAPE: 581.3891\n",
      "Test Data - Loss: 0.0013, MAE: 0.0353, MAPE: 92426.3828\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('Transformer+TimeEmbedding.hdf5',\n",
    "                                   custom_objects={'Time2Vector': Time2Vector, \n",
    "                                                   'SingleAttention': SingleAttention,\n",
    "                                                   'MultiAttention': MultiAttention,\n",
    "                                                   'TransformerEncoder': TransformerEncoder})\n",
    "\n",
    "\n",
    "#Calculate predication for training, validation and test data\n",
    "train_pred = model.predict(X_train)\n",
    "val_pred = model.predict(X_val)\n",
    "test_pred = model.predict(X_test)\n",
    "\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval = model.evaluate(X_train, y_train, verbose=0)\n",
    "val_eval = model.evaluate(X_val, y_val, verbose=0)\n",
    "test_eval = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(' ')\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
    "print('Validation Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(val_eval[0], val_eval[1], val_eval[2]))\n",
    "print('Test Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(test_eval[0], test_eval[1], test_eval[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "d42d320c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12.         12.         12.         ... 14.         13.\n",
      "  13.        ]\n",
      " [12.         12.         12.         ... 13.         13.\n",
      "  13.        ]\n",
      " [12.         12.         12.         ... 13.         13.\n",
      "  13.        ]\n",
      " ...\n",
      " [ 2.03999996  2.33999991  2.3900001  ...  1.50999999  1.5\n",
      "   1.62      ]\n",
      " [ 2.33999991  2.3900001   2.43000007 ...  1.5         1.62\n",
      "   1.63999999]\n",
      " [ 2.3900001   2.43000007  2.4000001  ...  1.62        1.63999999\n",
      "   1.88999999]]\n",
      "[[667.1186  811.92914 681.7896  ... 963.8876  850.8713  911.79675]\n",
      " [667.13794 811.9795  681.8116  ... 963.90985 850.8911  911.79926]\n",
      " [667.1383  812.0065  681.7478  ... 963.89685 850.8601  911.79205]\n",
      " ...\n",
      " [667.3059  812.104   681.9637  ... 964.0984  851.0137  911.942  ]\n",
      " [667.3102  812.0371  682.0063  ... 964.0705  851.06415 911.9086 ]\n",
      " [667.2904  812.0615  682.0264  ... 964.0731  851.06775 911.8886 ]]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "real_price = y_sc.inverse_transform(y_test)\n",
    "test_pred = y_sc.inverse_transform(test_pred)\n",
    "period_pred_accuracy = metrics.mean_absolute_percentage_error(real_price,test_pred)\n",
    "print(real_price)\n",
    "print(test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "4d980d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253.13395864611553\n"
     ]
    }
   ],
   "source": [
    "#30 days prediction error in total\n",
    "print(period_pred_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "1eb8ec41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n"
     ]
    }
   ],
   "source": [
    "#预测并输出预测值\n",
    "f_X_test = np.array([test_data[len(test_data)-seq_len:]])\n",
    "f_pred = model.predict(f_X_test)\n",
    "f_pred = y_sc.inverse_transform(f_pred)\n",
    "f_pred = [item[i] for item in f_pred for i in range(30)]\n",
    "date_index = [df.index[-1]+datetime.timedelta(days=i) for i in range(1,30+1)]\n",
    "prediction_list = pd.DataFrame(f_pred,index = date_index,columns=['Prediction Pirce'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "e066c904",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_output = df[['Close','Volume']]\n",
    "data_output = pd.merge(data_output,prediction_list,left_index=True,right_index=True,how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "451bb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/ningxuying/Desktop/Data/'+str(stock_ticket)+'变形金刚.csv'\n",
    "data_output.to_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "40031ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "randomclassifier = joblib.load('/Users/ningxuying/Desktop/model/SentimentAnalysis.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "2dcb61f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scraping\n",
    "from urllib.request import Request, urlopen\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import pandas as pd\n",
    "\n",
    "# Let's pick a company ticker symbol (AMZN for Amazon)\n",
    "company_ticker = stock_ticket\n",
    "# Add the ticker symbol to the \"finviz\" search box url\n",
    "url = (\"http://finviz.com/quote.ashx?t=\" + company_ticker.lower())\n",
    "# Most websites block requests that are without a User-Agent header (these simulate a typical browser)\n",
    "\n",
    "# Send a Request to the url and return an html file\n",
    "req = Request(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "\n",
    "# open and read the request\n",
    "webpage = urlopen(req).read()\n",
    "\n",
    "# make a soup using BeautifulSoup from webpage\n",
    "html = soup(webpage, \"html.parser\")\n",
    "\n",
    "# Extract the 'class' = 'fullview-news-outer' from our html code, and create a dataframe from it\n",
    "news = pd.read_html(str(html), attrs={'class': 'fullview-news-outer'})[0]\n",
    "# extract the links for each news by finding all the \"a\" tags and 'class' = 'tab-link-news'\n",
    "links = []\n",
    "for a in html.find_all('a', class_=\"tab-link-news\"):\n",
    "    links.append(a['href'])\n",
    "\n",
    "# Clean up our news dataframe\n",
    "news.columns = ['Date', 'News_Headline']\n",
    "news['Article_Link'] = links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "38671729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backtest Result\n",
      "Invest Length: 30 Days\n",
      "Total Investment Times: 17 \n",
      "Times That has the Right Decision: 7 \n",
      "Rate of the Right Decision: 0.4117647058823529\n",
      "Net Return: -0.5499997138977051\n",
      "Return Rate: -0.2696077079325603\n"
     ]
    }
   ],
   "source": [
    "#backtest for 30 days!\n",
    "#backtest 1\n",
    "print(\"Backtest Result\")\n",
    "predict_price = test_pred_price['Prediction Pirce']\n",
    "invest_length = 30\n",
    "real_price = df['Close'][-30-1:]\n",
    "real_open_price = df['Open'][-30-1:]\n",
    "print(\"Invest Length:\",invest_length,\"Days\")\n",
    "list(predict_price).insert(0,df['Close'][-30-1])\n",
    "buy = 0\n",
    "net_return = 0\n",
    "right_decision=0\n",
    "#预测出来下一天涨，就买入\n",
    "#预测出来下一天涨，就买入\n",
    "for i in range(1,30):\n",
    "    if predict_price[i] > predict_price[i-1]:\n",
    "        buy = buy +1\n",
    "        net_return = net_return + (real_price[i+1]-real_open_price[i])\n",
    "        if real_price[i] > real_price[i-1]:\n",
    "            right_decision = right_decision+1\n",
    "print(\"Total Investment Times:\",buy,\"\\nTimes That has the Right Decision:\",right_decision,\"\\nRate of the Right Decision:\",right_decision/buy)\n",
    "# print(buy)\n",
    "print(\"Net Return:\",net_return)\n",
    "return_rate = net_return/real_open_price[0]\n",
    "print(\"Return Rate:\",return_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "35bd509f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#other to save\n",
    "other_data = {'ticket':[stock_ticket],\n",
    "    'Backtest Length':[invest_length],\n",
    "              'Total Investment Times':[buy],\n",
    "              'Rate of the Right Decision':[right_decision],\n",
    "              'Backtest Net Return':[net_return],\n",
    "              \"Backtest Return Rate(%)\":[return_rate*100],\n",
    "               \"Next Day's Predicted Price\":[prediction_list['Prediction Pirce'][0]],\n",
    "              \"Daily Stock Price Prediction Accuracy(%)\":['NULL'],\n",
    "             \"30 Day's Stock Price Prediction Accuracy(%)\":[100-period_pred_accuracy*100],\n",
    "             'Expected Net Return in Future 30 Days':[prediction_list['Prediction Pirce'][-1]-df['Close'][-1]],\n",
    "             \"Expected Return Rate in Future 30 Days(%)\":[(prediction_list['Prediction Pirce'][-1]-df['Close'][-1])/df['Close'][-1]*100]}\n",
    "news_data = {'News Dates':news['Date'][:5],\n",
    "             'News Headlines':news['News_Headline'][:5],\n",
    "             'News Links':news['Article_Link'][:5]}\n",
    "other_data = pd.DataFrame(other_data)\n",
    "news_data = pd.DataFrame(news_data)\n",
    "other_data = pd.concat([other_data,news_data],axis = 1,join = 'outer')\n",
    "od_path = '/Users/ningxuying/Desktop/OD/'+str(stock_ticket)+'变形金刚.csv'\n",
    "other_data.to_csv(od_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e023e49d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b914ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
